{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anonimiza√ß√£o Segmentada com face_anon_simple\n",
        "\n",
        "Este notebook demonstra como usar o modelo `face_anon_simple` (diffusion) para anonimizar apenas regi√µes espec√≠ficas do rosto, como olhos, boca, nariz, etc.\n",
        "\n",
        "A anonimiza√ß√£o √© aplicada apenas nas partes segmentadas escolhidas, mantendo o resto da face inalterado.\n",
        "\n",
        "## ‚ö†Ô∏è Importante para Google Colab\n",
        "\n",
        "Se voc√™ estiver usando o Google Colab, certifique-se de que os diret√≥rios `src/` e `utils/` est√£o dispon√≠veis:\n",
        "\n",
        "1. **Op√ß√£o 1 - Upload manual**: Fa√ßa upload dos diret√≥rios `src/` e `utils/` para o Colab\n",
        "2. **Op√ß√£o 2 - Git clone**: Se o projeto estiver no GitHub, use `!git clone <repo_url>`\n",
        "3. **Op√ß√£o 3 - Google Drive**: Monte o Google Drive e aponte para o diret√≥rio do projeto\n",
        "\n",
        "O notebook tentar√° detectar automaticamente o caminho correto, mas voc√™ pode precisar ajustar manualmente se necess√°rio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configura√ß√£o do Ambiente (Opcional - apenas para Colab)\n",
        "\n",
        "Se voc√™ estiver no Google Colab e os arquivos n√£o estiverem dispon√≠veis, use uma das op√ß√µes abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# OP√á√ÉO 1: Clonar do GitHub (se o projeto estiver no GitHub)\n",
        "# ============================================\n",
        "# Descomente e ajuste a URL do reposit√≥rio:\n",
        "# !git clone https://github.com/seu-usuario/face_anon_simple.git\n",
        "# %cd face_anon_simple\n",
        "\n",
        "# ============================================\n",
        "# OP√á√ÉO 2: Montar Google Drive\n",
        "# ============================================\n",
        "# Se seus arquivos est√£o no Google Drive:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/caminho/para/face_anon_simple\n",
        "\n",
        "# ============================================\n",
        "# OP√á√ÉO 3: Upload manual via interface do Colab\n",
        "# ============================================\n",
        "# Use o painel lateral do Colab para fazer upload dos diret√≥rios src/ e utils/\n",
        "\n",
        "print(\"‚ö†Ô∏è Esta c√©lula √© opcional. Execute apenas se precisar configurar o ambiente no Colab.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting face_alignment\n",
            "  Downloading face_alignment-1.4.1-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from face_alignment) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face_alignment) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.12/dist-packages (from face_alignment) (1.16.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from face_alignment) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from face_alignment) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from face_alignment) (4.67.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from face_alignment) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->face_alignment) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->face_alignment) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->face_alignment) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->face_alignment) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->face_alignment) (3.0.3)\n",
            "Downloading face_alignment-1.4.1-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: face_alignment\n",
            "Successfully installed face_alignment-1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install face_alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3716347189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferencenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferencenet_unet_2d_condition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReferenceNetModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferencenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet_2d_condition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNet2DConditionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferencenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_referencenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionReferenceNetPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import CLIPImageProcessor, CLIPVisionModel\n",
        "from diffusers import AutoencoderKL, DDPMScheduler\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "import face_alignment\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from src.diffusers.models.referencenet.referencenet_unet_2d_condition import ReferenceNetModel\n",
        "from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel\n",
        "from src.diffusers.pipelines.referencenet.pipeline_referencenet import StableDiffusionReferenceNetPipeline\n",
        "\n",
        "from utils.segmented_anonymization import anonymize_faces_segmented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregar Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "face_model_id = \"hkung/face-anon-simple\"\n",
        "clip_model_id = \"openai/clip-vit-large-patch14\"\n",
        "sd_model_id = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "print(f\"Device: {device}, dtype: {dtype}\")\n",
        "\n",
        "print(\"Carregando UNet...\")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    face_model_id, subfolder=\"unet\", use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Carregando ReferenceNet...\")\n",
        "referencenet = ReferenceNetModel.from_pretrained(\n",
        "    face_model_id, subfolder=\"referencenet\", use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Carregando Conditioning ReferenceNet...\")\n",
        "conditioning_referencenet = ReferenceNetModel.from_pretrained(\n",
        "    face_model_id, subfolder=\"conditioning_referencenet\", use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Carregando VAE...\")\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    sd_model_id, subfolder=\"vae\", use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Carregando Scheduler...\")\n",
        "scheduler = DDPMScheduler.from_pretrained(\n",
        "    sd_model_id, subfolder=\"scheduler\", use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Carregando CLIP...\")\n",
        "feature_extractor = CLIPImageProcessor.from_pretrained(\n",
        "    clip_model_id, use_safetensors=True\n",
        ")\n",
        "image_encoder = CLIPVisionModel.from_pretrained(\n",
        "    clip_model_id, use_safetensors=True\n",
        ")\n",
        "\n",
        "print(\"Criando pipeline...\")\n",
        "pipe = StableDiffusionReferenceNetPipeline(\n",
        "    unet=unet,\n",
        "    referencenet=referencenet,\n",
        "    conditioning_referencenet=conditioning_referencenet,\n",
        "    vae=vae,\n",
        "    feature_extractor=feature_extractor,\n",
        "    image_encoder=image_encoder,\n",
        "    scheduler=scheduler,\n",
        ")\n",
        "pipe = pipe.to(device, dtype=dtype)\n",
        "\n",
        "print(\"‚úì Pipeline carregado com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inicializar Face Alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fa = face_alignment.FaceAlignment(\n",
        "    face_alignment.LandmarksType.TWO_D,\n",
        "    face_detector=\"sfd\",\n",
        "    device=device\n",
        ")\n",
        "print(\"‚úì Face Alignment inicializado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carregar Imagem de Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"my_dataset/test/00482.png\"\n",
        "original_image = load_image(image_path)\n",
        "print(f\"‚úì Imagem carregada: {original_image.size}\")\n",
        "\n",
        "# Visualizar imagem original\n",
        "display(original_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Definir Casos de Teste\n",
        "\n",
        "Definimos diferentes combina√ß√µes de caracter√≠sticas faciais para testar a anonimiza√ß√£o segmentada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        'features': ['eyes'],\n",
        "        'name': 'Apenas Olhos',\n",
        "        'num_steps': 30,  # Menos passos para teste mais r√°pido\n",
        "    },\n",
        "    {\n",
        "        'features': ['mouth'],\n",
        "        'name': 'Apenas Boca',\n",
        "        'num_steps': 30,\n",
        "    },\n",
        "    {\n",
        "        'features': ['eyes', 'mouth'],\n",
        "        'name': 'Olhos + Boca',\n",
        "        'num_steps': 30,\n",
        "    },\n",
        "    {\n",
        "        'features': ['nose'],\n",
        "        'name': 'Nariz',\n",
        "        'num_steps': 30,\n",
        "    },\n",
        "    {\n",
        "        'features': ['eyebrows', 'eyes'],\n",
        "        'name': 'Sobrancelhas + Olhos',\n",
        "        'num_steps': 30,\n",
        "    },\n",
        "    {\n",
        "        'features': ['eyes', 'nose', 'mouth'],\n",
        "        'name': 'Olhos + Nariz + Boca',\n",
        "        'num_steps': 30,\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Total de casos de teste: {len(test_cases)}\")\n",
        "for i, case in enumerate(test_cases):\n",
        "    print(f\"  {i+1}. {case['name']}: {case['features']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Executar Anonimiza√ß√£o Segmentada\n",
        "\n",
        "Agora vamos aplicar a anonimiza√ß√£o usando `face_anon_simple` apenas nas regi√µes segmentadas escolhidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "generator = torch.manual_seed(42)\n",
        "\n",
        "for i, test_case in enumerate(test_cases):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Teste {i+1}/{len(test_cases)}: {test_case['name']}\")\n",
        "    print(f\"Features: {test_case['features']}\")\n",
        "    print(f\"Passos de infer√™ncia: {test_case['num_steps']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    try:\n",
        "        anon_image = anonymize_faces_segmented(\n",
        "            image=original_image,\n",
        "            face_alignment_model=fa,\n",
        "            mask_features=test_case['features'],\n",
        "            operator_type='diffusion',\n",
        "            pipe=pipe,\n",
        "            generator=generator,\n",
        "            num_inference_steps=test_case['num_steps'],\n",
        "            guidance_scale=4.0,\n",
        "            anonymization_degree=1.25,\n",
        "            dilate_radius=3,\n",
        "            smooth_edges=True,\n",
        "        )\n",
        "        \n",
        "        # Salvar resultado\n",
        "        safe_name = test_case['name'].replace(' ', '_').replace('+', '_').lower()\n",
        "        output_path = f\"test_diffusion_segmented_{i+1:02d}_{safe_name}.png\"\n",
        "        anon_image.save(output_path)\n",
        "        \n",
        "        print(f\"‚úì Anonimiza√ß√£o conclu√≠da\")\n",
        "        print(f\"‚úì Salvo em: {output_path}\")\n",
        "        \n",
        "        # Visualizar resultado\n",
        "        display(anon_image)\n",
        "        \n",
        "        results.append({\n",
        "            'name': test_case['name'],\n",
        "            'features': test_case['features'],\n",
        "            'output': output_path,\n",
        "            'image': anon_image,\n",
        "            'status': 'success'\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Erro: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        results.append({\n",
        "            'name': test_case['name'],\n",
        "            'status': 'error',\n",
        "            'error': str(e)\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Resumo e Compara√ß√£o\n",
        "\n",
        "Criamos um grid com todos os resultados para compara√ß√£o visual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo dos resultados\n",
        "successful = [r for r in results if r.get('status') == 'success']\n",
        "failed = [r for r in results if r.get('status') == 'error']\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RESUMO DOS TESTES\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úì Sucessos: {len(successful)}/{len(test_cases)}\")\n",
        "print(f\"‚úó Falhas: {len(failed)}/{len(test_cases)}\")\n",
        "\n",
        "if successful:\n",
        "    print(\"\\n‚úì Testes conclu√≠dos com sucesso:\")\n",
        "    for r in successful:\n",
        "        print(f\"  - {r['name']}: {r['output']}\")\n",
        "        print(f\"    Features: {r['features']}\\n\")\n",
        "\n",
        "if failed:\n",
        "    print(\"\\n‚úó Erros encontrados:\")\n",
        "    for r in failed:\n",
        "        print(f\"  - {r['name']}: {r.get('error', 'Erro desconhecido')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar grid de compara√ß√£o\n",
        "if len(successful) > 0:\n",
        "    result_images = [r['image'] for r in successful]\n",
        "    \n",
        "    # Adicionar imagem original no in√≠cio\n",
        "    result_images.insert(0, original_image)\n",
        "    \n",
        "    grid = make_image_grid(result_images, rows=2, cols=4)\n",
        "    grid.save(\"test_diffusion_segmented_comparison.png\")\n",
        "    \n",
        "    print(\"‚úì Grid de compara√ß√£o criado!\")\n",
        "    display(grid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nota Importante\n",
        "\n",
        "A anonimiza√ß√£o com `face_anon_simple` foi aplicada **apenas nas regi√µes segmentadas especificadas**. O resto da face permanece inalterado, demonstrando anonimiza√ß√£o seletiva.\n",
        "\n",
        "Isso permite:\n",
        "- Anonimizar apenas caracter√≠sticas espec√≠ficas (ex: apenas olhos)\n",
        "- Manter outras partes do rosto vis√≠veis\n",
        "- Combinar diferentes caracter√≠sticas (ex: olhos + boca)\n",
        "- Controlar precisamente quais partes s√£o anonimizadas\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
