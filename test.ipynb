{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented Facial Anonymization Ablation Study\n",
    "\n",
    "This notebook demonstrates the segmented facial anonymization with 3 masks and 3 operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModel\n",
    "from diffusers import AutoencoderKL, DDPMScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "import face_alignment\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from src.diffusers.models.referencenet.referencenet_unet_2d_condition import ReferenceNetModel\n",
    "from src.diffusers.models.referencenet.unet_2d_condition import UNet2DConditionModel\n",
    "from src.diffusers.pipelines.referencenet.pipeline_referencenet import StableDiffusionReferenceNetPipeline\n",
    "\n",
    "from utils.segmented_anonymization import anonymize_faces_segmented\n",
    "from utils.segmentation import get_mask_from_landmarks, visualize_mask, get_segmented_regions\n",
    "from utils.extractor import get_transform_mat, FaceType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model_id = \"hkung/face-anon-simple\"\n",
    "clip_model_id = \"openai/clip-vit-large-patch14\"\n",
    "sd_model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(face_model_id, subfolder=\"unet\", use_safetensors=True)\n",
    "referencenet = ReferenceNetModel.from_pretrained(face_model_id, subfolder=\"referencenet\", use_safetensors=True)\n",
    "conditioning_referencenet = ReferenceNetModel.from_pretrained(face_model_id, subfolder=\"conditioning_referencenet\", use_safetensors=True)\n",
    "vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder=\"vae\", use_safetensors=True)\n",
    "scheduler = DDPMScheduler.from_pretrained(sd_model_id, subfolder=\"scheduler\", use_safetensors=True)\n",
    "feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id, use_safetensors=True)\n",
    "image_encoder = CLIPVisionModel.from_pretrained(clip_model_id, use_safetensors=True)\n",
    "\n",
    "pipe = StableDiffusionReferenceNetPipeline(\n",
    "    unet=unet,\n",
    "    referencenet=referencenet,\n",
    "    conditioning_referencenet=conditioning_referencenet,\n",
    "    vae=vae,\n",
    "    feature_extractor=feature_extractor,\n",
    "    image_encoder=image_encoder,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "pipe = pipe.to(device, dtype=dtype)\n",
    "\n",
    "# Initialize Face Alignment\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, face_detector=\"sfd\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Masks and Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98aa3e8",
   "metadata": {},
   "source": [
    "## Visualizar Máscaras de Segmentação\n",
    "\n",
    "Esta seção demonstra como visualizar as máscaras de segmentação facial antes de aplicar a anonimização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa90db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar imagem de teste\n",
    "image_path = \"my_dataset/test/14795.png\"\n",
    "original_image = load_image(image_path)\n",
    "image_np = np.array(original_image)\n",
    "\n",
    "# Garantir RGB\n",
    "if image_np.shape[2] == 4:\n",
    "    image_np = image_np[:, :, :3]\n",
    "\n",
    "# Detectar landmarks\n",
    "preds = fa.get_landmarks(image_np)\n",
    "\n",
    "if preds is not None:\n",
    "    # Visualizar máscaras para cada rosto detectado\n",
    "    face_image_size = 512\n",
    "    \n",
    "    for idx, landmarks in enumerate(preds):\n",
    "        # Obter matriz de transformação\n",
    "        mat = get_transform_mat(landmarks, face_image_size, FaceType.WHOLE_FACE)\n",
    "        \n",
    "        # Extrair face alinhada\n",
    "        face_aligned = cv2.warpAffine(\n",
    "            image_np,\n",
    "            mat,\n",
    "            (face_image_size, face_image_size),\n",
    "            cv2.INTER_LANCZOS4,\n",
    "            borderValue=(255, 255, 255),\n",
    "        )\n",
    "        \n",
    "        # Transformar landmarks para o espaço alinhado\n",
    "        pts = np.array([landmarks], dtype=np.float32)\n",
    "        aligned_landmarks = cv2.transform(pts, mat)[0]\n",
    "        \n",
    "        # Visualizar máscaras definidas\n",
    "        print(f\"Visualizando máscaras para rosto {idx + 1}:\")\n",
    "        visualization_images = []\n",
    "        \n",
    "        for mask_name, features in masks.items():\n",
    "            # Gerar máscara\n",
    "            mask = get_mask_from_landmarks(\n",
    "                aligned_landmarks,\n",
    "                (face_image_size, face_image_size),\n",
    "                features,\n",
    "                dilate_radius=3,\n",
    "                smooth_edges=True\n",
    "            )\n",
    "            \n",
    "            # Visualizar máscara sobreposta na face alinhada\n",
    "            vis_image = visualize_mask(face_aligned, mask, alpha=0.5)\n",
    "            visualization_images.append(Image.fromarray(vis_image))\n",
    "            print(f\"  - {mask_name}: {features}\")\n",
    "        \n",
    "        # Criar grid de visualizações\n",
    "        if visualization_images:\n",
    "            grid = make_image_grid(visualization_images, rows=1, cols=len(masks))\n",
    "            display(grid)\n",
    "            print(f\"\\nMáscaras visualizadas para rosto {idx + 1}\\n\")\n",
    "else:\n",
    "    print(\"Nenhum rosto detectado na imagem.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks definition\n",
    "# (i) eyes + mouth + nostrils\n",
    "mask1 = ['eyes', 'mouth', 'nostrils']\n",
    "\n",
    "# (ii) + eyebrows\n",
    "mask2 = mask1 + ['eyebrows']\n",
    "\n",
    "# (iii) + lip contour + teeth\n",
    "# Note: 'mouth' in mask1 typically covers lips and teeth if it refers to the outer lip boundary.\n",
    "# If we want to be explicit or if 'mouth' meant something else, we add them.\n",
    "# Here we add 'lips' and 'teeth' explicitly, though they might be redundant if 'mouth' is full.\n",
    "# To ensure we cover everything requested:\n",
    "mask3 = mask2 + ['lips', 'teeth']\n",
    "\n",
    "masks = {\n",
    "    \"Mask 1 (Eyes+Mouth+Nostrils)\": mask1,\n",
    "    \"Mask 2 (+Eyebrows)\": mask2,\n",
    "    \"Mask 3 (+Lips+Teeth)\": mask3\n",
    "}\n",
    "\n",
    "operators = ['blur', 'mosaic', 'diffusion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image\n",
    "image_path = \"my_dataset/test/14795.png\" # Using an example from the repo\n",
    "original_image = load_image(image_path)\n",
    "\n",
    "results = []\n",
    "labels = []\n",
    "\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "for mask_name, features in masks.items():\n",
    "    row_images = []\n",
    "    for op in operators:\n",
    "        print(f\"Processing {mask_name} with {op}...\")\n",
    "        anon_image = anonymize_faces_segmented(\n",
    "            image=original_image,\n",
    "            face_alignment_model=fa,\n",
    "            mask_features=features,\n",
    "            operator_type=op,\n",
    "            pipe=pipe,\n",
    "            generator=generator,\n",
    "            # Operator specific params\n",
    "            kernel_size=(31, 31), # For blur\n",
    "            block_size=15,        # For mosaic\n",
    "            num_inference_steps=30, # For diffusion (faster for demo)\n",
    "            guidance_scale=4.0,\n",
    "            anonymization_degree=1.25\n",
    "        )\n",
    "        row_images.append(anon_image)\n",
    "        labels.append(f\"{mask_name}\\n{op}\")\n",
    "    results.extend(row_images)\n",
    "\n",
    "# Display results\n",
    "grid = make_image_grid(results, rows=3, cols=3)\n",
    "grid.save(\"ablation_results.png\")\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
